\chapter{Architecture}
\label{chapter:3}
\section{Overview}
In this chapter the overview of proposed system extension design will be discussed.
The proposed system was designed using top-down approach. First the user interface with regards to usability and Docker ecosystem, then the program managing the simulation and ways of communication with user interface that are most suitable for system use cases. Finally, the methods of connecting existing simulation programs with the managing program, and how to share data between them.
\section{User interface}
At very top of the system the top there is the user, that needs some interface to interact with the system. The current interface to each component is \textit{GUI} that is not suited for non-technical user, so there is a strong inclination to build a graphical interface. The two main kinds are native application with \textit{GUI} and \textit{WEB-GUI}. Thought the \textit{GUI} of operating system inside of a docker container can be shared to host through \textit{VNC} server or \textit{Xauthority} \cite{DockerGUI}, it is much easier to host simple http server that exposes a port to host, and can be accessed through web browser. For this reason, \textit{WEB-GUI} interface was chosen as the preferred method of interacting with user. This part of the proposed will be refereed in this thesis as \textit{SimBaD-Client}. This first design iteration can be seen on the figure \ref{fig:arch0}. It is important to underline that single component on diagram does not necessarily equal single docker container, and the exact separation of SimBaD project components into containers will be discussed in \ref{section:containers}.
\begin{figure}[h!]
	\centering
		\includegraphics[width=0.9\linewidth]{diagrams/arch1.png}
	\caption{Architecture Overview: SimBaD Client}
	\label{fig:arch0}
\end{figure}
\section{Managing the simulation}
Another component that is necessary is a component that will connect all of the simulation pipeline steps together. This component should receive and execute commands from \textit{SimBaD-Client}. The use of web client narrows the possible methods of communications to network protocols and forces the component that receives commands to have \textit{HTTP} capabilities. Such component in this this will be referred to as \textit{SimBaD-Pipeline-Server} or \textit{SPS}. 
\subsection{Methods of communication}
There are several methods on server-client communications to choose from, in the following section the \textit{WebSocket}, \textit{REST API} and \textit{GraphQL} will be discussed.

\textit{WebSocket} is a communication protocol that provides full-duplex (data can be send both ways at the same time) communication over TCP connection \cite{Melnikov2011}. From client standpoint it provides server push capabilities - client can receive updates from server without explicitly asking for them. This makes it more suitable for real-time communication. Because WebSockets reuse single TCP connection, they have very low overhead - after initial handshake there is no need to send such data as request headers or cookies. This also leads to very high throughput. 

The \textit{REST} (Representational State Transfer) is an style of software architecture for creating web services \cite{Fielding2014}. Rest puts some constraints on a communication protocols, such as\textit{HTTP}. Using REST it is possible to define communication endpoint (an exposed interface) and operations, (usually \textit{CRUD}) that can be made using that endpoint. The major benefit of \textit{REST} is it allows to define very transparent interfaces. \textit{REST} is a mature technology, and as such, it is very standardized with such standards as \textit{OpenApi} and that helps with development and documentation. \textit{REST} is most suitable when client-server interactions are less frequent. One of the principles of \textit{REST} architecture is client-server split - it defines a contract that both side of communication need to keep, thus allowing separate development of client and server. The obvious disadvantage of this approach is that it does not allow for real-time data to be passed to client. When clients needs to have updates from server, it has to continuously ask for it - such process is also called long polling. Operations can be defined as precisely as needed, but they nevertheless they need to be defined - it is common to get whole object to get information about its property which causes unnecessary data to be send, and to get only this property, separate endpoint needs to be defined. Such problem is also called over-fetching. The \textit{REST} also suffers from the opposite problem of under-fetching, where to get necessary the client needs to make more than one request.

\textit{GraphQL} or Graph Query Language, was developed by \textit{Facebook}, to address the under-fetching and over-fetching issues without without writing additional endpoints. It is a relatively new technology, as the specification of it was open-sourced in 2015 \cite{GraphQL}. From client side it allows to define a structure of required data and from server side it allows to return specified data. In contrast to \textit{REST}, where the structure of the response is defined by endpoint, in Graphql the structure of response is created by \textit{GraphQL} engine to mach the structure of query. It supports the usual read/write \textit{CRUD} operations using mutations along with real-time data updates using subscriptions. The downside of this solution is that is a s it is relatively new, its ecosystem and tools are fewer and less mature compared to \textit{REST}. It is less suitable for simple \textit{APIs} as it adds a lot of complexity \cite{GraphQlNoNeed}.

To chose one, it is necessary to analyze the needs of the system for sending data. The user should trigger simulation by sending configuration file, and this would be the main case where the system needs to handle user input. After that, the client needs to get update from server about simulation progress - this would be the most common operation. Because the simulation can go on a very long time, there's no need for frequent updates, as changes to simulation state occur rarely. When simulation step or simulation as a whole finishes, user needs to be able to view and download its results. The WebSocket does not really add anything of value to the system, as data does not need to be real time, the updates are rare, and keeping single connection open for several days is not something one should do. \textit{REST API} is suited for above use-cases, and due to its standardization and tooling its the easiest one to develop with. \textit{GraphQL} while promising, is a slight overkill for such simple system, as the problems that it solves are not as common, however, this solution will become more viable with system growth. In conclusion, the \textit{REST API} was chosen as preferred communication between system components. On the figure \ref{fig:arch2}, diagram from figure \ref{fig:arch2} with added SPS and communication methods is shown. 
\begin{figure}[h!]
	\centering
		\includegraphics[width=0.9\linewidth]{diagrams/arch2.png}
	\caption{Architecture Overview: client-server communication}
	\label{fig:arch2}
\end{figure}
\newpage 
\subsection{API definitions}
All of the \textit{API} interfaces defined in proposed system, were defined using the \textit{OpenApi} standard. An example of such definition for operation that starts simulation process is visible on listing \ref{list:openapi-route}. \textit{OpenApi} definition consist of route, operations allowed on this route, in this case \textit{POST} operation, the request data and possible server responses - data and \textit{HTTP} codes. Such definition defines a contract that both sides client and server must keep. To help with development, \textit{OpenAPI} allows to generate server and client code from such definition, which considerably speeds up the development process and prototyping. With code generator, using few commands one can generate client \textit{API} code, server and mock server endpoints, and as long as the contract is kept, both sides of can be developed independent of each other. The endpoints defined using this standard are very well documented, and easy to use for developers. Additional tools such as \textit{Swagger Editor} can be used to create and visualize those definitions. 
\begin{lstlisting}[label=list:openapi-route,caption=The \textit{OpenAPI} route definition for simulation start request, basicstyle=\footnotesize\ttfamily]
paths:
  /api/simulation/start:
    post:
      tags:
        - SIMULATION
      operationId: startSimulation
      description: Starts simulation process
      requestBody:
        content:
          application:/json:
            schema:
              $ref: "#/components/schemas/StartSimulationRequest"
      responses:
        "202":
          description: Request is accepted and command starts to execute
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/SimulationStepInfo"
        "400":
          description: Simulation process is already running
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/SimulationError"
components:
  schemas:
    StartSimulationRequest:
      type: object
      properties:
        configurationName:
          description: The configuration file name
          type: string
        configuration:
          description: The configuration file to be sent to CLI
          type: string
          format: binary
      required:
        - configuration
        - configurationName
\end{lstlisting}
\subsubsection{Sharing and persisting data}
The first step of devising a proper way to share data for such pipeline system is to figure out how data should be passed between components. In its current state, the components do not pass whole files directly to each other, but rather pass the paths to files. The consequence of this is that each component needs to have access to filesystem where the file resides, and this in turn poses a question: how to give access to the same filesystem to multiple separate components? Docker solves such problem using Docker volumes. Though container data can also be stored in the top read-write layer, such data is deleted when container is deleted \cite{DockerStorage}. Docker volumes are a way to persist and share data between containers or between containers and host. The program running inside of container sees such volume as a folder, and can write and read data to it. Same volume can be used by multiple docker containers, solving the problem of shared filesystem. Apart from existence of actual simulation data on the filesystem, some kind of additional metadata should be specified - such information as the path to output file or which file is the result of which simulation or step. Though such information can also be stored in the filesystem, for example in some kind of \textit{CSV} or \textit{JSON} file, it is more convenient to store them in database. As the writes and reads to the database are sparse, and the data written is of small size, \textit{SQLite} database was chosen. The data in \textit{SQLite} database is stored in single \textit{SQLITE} file, that can also be placed on the shared volume. This choice also reduces the \textit{Docker} memory footprint on system, as other databases like \textit{MySQL} or \textit{Postgress} would require separate container to work. On the figure \ref{fig:arch4}, the architecture diagram extension of figure \ref{fig:arch2} with existing components and the data-flow between components that use shared volume is shown.
\begin{figure}[h!]
	\centering
		\includegraphics[width=0.9\linewidth]{diagrams/arch4.png}
	\caption{Architecture Overview: shared volume data flow}
	\label{fig:arch4}
\end{figure}
\section{Extending existing components}
The usual way to communicate between docker containers is to use networking. As existing components do not have networking capabilities, the only viable way to connect all of them is to place them in single container, so the \textit{SPS} can issue command by directly executing components scripts and binaries. This however does not fulfill the modularity and configurability goals, because it prevents components from being decoupled and enforces single viable configuration. Modifying components to add networking features is also out of the picture, as that would violate the non-invasive integration constraint. The solution that satisfies all that is to add simple \textit{HTTP} server on top of each component that should be able to independent. This layer should allow to trigger the execution, get the progress, and get the result of step. Such extension was added to \textit{SimBaD-Analyzer} step. Through use of \textit{SSH} tunnel forwarding, this extension can also function over \textit{SSH} connections \cite{SSHTunnel}.
\section{Summary}
In the proposed architecture, the user will control and monitor the simulation process with web client - \textit{SimBaD-Client}. The client, using \textit{REST API} will issue commands to the server \textit{(SimBaD-Pipeline-Server)}, and the server will connect all of the simulation components together and manage simulation process. To connect all of the simulation steps to server, two methods will be used: storing simulation step scripts and binaries on the same container as server, or for separate containers, adding \textit{HTTP} server on top of simulation step component, without modifying it. Such extension \textit{SimBaD-Analyzer-Server} was added to the \textit{SimBaD-Analyzer} step.